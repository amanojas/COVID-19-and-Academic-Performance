% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \setmainfont[]{Palatino}
  \setsansfont[]{Palatino}
  \setmonofont[]{Courier New}
  \setmathfont[]{Palatino}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{setspace}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{endfloat}
\usepackage{threeparttable}
\usepackage{siunitx}
\sisetup{detect-all=true}
\setlength{\defaultaddspace}{0pt}
\providecommand\makecell[2][]{\begin{tabular}{@{}c@{}}#2\end{tabular}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Effects of COVID-19 on the Academic Performance of College Students},
  pdfauthor={Aman Desai},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}


\title{Effects of COVID-19 on the Academic Performance of College
Students}
\author{Aman Desai}
\date{Feb 10, 2026}
\begin{document}
\maketitle
\begin{abstract}
This study examines the impact of the COVID-19 pandemic on undergraduate
performance in an introductory microeconomics course at a large public
university. A key challenge in assessing academic outcomes during this
period lies in disentangling true learning effects from institutional
responses such as flexible grading policies and instructor adaptations
designed to mitigate the disruptions of the sudden shift to remote
instruction. To address these confounds, I analyze raw scores on a
common departmental final examination administered across all sections
from 2019 through Spring 2022. To further control for potential
variation in exam difficulty, I construct a granular dataset matching
student performance on identical or nearly identical questions appearing
in both pre-pandemic and pandemic-era exams. The results indicate that
adjusted mean scores on the common final declined by 1.15 points, while
the probability of answering a matched question correctly fell by 1.9
percentage points. Heterogeneity analyses reveal that these aggregate
declines mask significant distributional effects: students with
below-median GPAs experienced an additional 3.3 percentage point
reduction in the probability of answering matched questions correctly
relative to their high-GPA peers. Conversely, the abrupt transition to
remote instruction was associated with a 6 percentage point increase in
the probability of answering matched questions correctly for online
students relative to the pre-pandemic hybrid baseline.

\vspace{12pt}

\noindent \textbf{Keywords:} COVID-19, academic performance,
undergraduate education, online learning
\end{abstract}


\setstretch{2}
\pagebreak

\section{Introduction}\label{introduction}

The COVID-19 pandemic of March 2020 was disruptive across many domains,
with higher education being one of them. Policies were implemented
worldwide in response to this global crisis, resulting in changes in the
educational setting. Educational instructions were abruptly moved online
without prior preparation. This had a negative effect on primary and
secondary education, leading to significant learning loss for students
(\citeproc{ref-grewenig_covid-19_2021}{Grewenig et al. 2021};
\citeproc{ref-fuchs-schundeln_covid-induced_2022}{Fuchs-Schündeln
2022}).

Although the socioeconomic consequences of COVID-19 have been
extensively studied from various perspectives, research on the impact of
the pandemic on college students remains limited and yields conflicting
results. Most studies examining the impact of the pandemic on students'
academic performance measure outcomes such as GPA and course completion
rates. Although useful, these measures are confounded by the numerous
responses of students, faculty, and administrators during the pandemic.
For example, cheating became a challenging issue in the rapid move to
online teaching (\citeproc{ref-ives_did_2024}{Ives and Cazan 2024};
\citeproc{ref-jenkins_when_2023}{Jenkins et al. 2023};
\citeproc{ref-walsh_why_2021}{Walsh et al. 2021}). The faculty adopted
more lenient grading practices and reduced exam difficulties.
Administrators altered grading policies regarding course withdrawals and
pass/fail options
(\citeproc{ref-rodriguez-planas_covid-19_2022}{Rodríguez-Planas 2022}).
These responses made comparisons with pre-pandemic test scores, or the
term GPA less reliable for quantifying learning loss during the pandemic
in college students.

I address these assessment and grading issues using unique exam-level
data from a large public university in New York City. First, I analyze
students' performance on final exams before and during the pandemic in
an \emph{introductory microeconomics} course. Approximately 800 students
across ten sections of the course attempted a common final exam each
semester. This reduces the variation in the difficulty of exams across
sections. However, the difficulty of an exam may have changed in
response to the pandemic. Thus, I compare students' performance on
specific exam questions that were qualitatively almost identical before
and during the pandemic by matching questions from answer sheets from
the common final exams before and during the pandemic. By focusing on
students' performance on nearly identical questions before and during
the pandemic, I remove the variation in outcomes due to possible changes
in the difficulty of these exams during the pandemic. By combining the
matched question-level data with student characteristics, I estimate how
the pandemic affected students' average probability of correctly
answering similar questions from pre-pandemic common exams during the
crisis.

I begin with a before and after analysis, adjusting for student
characteristics, time periods, and instructor fixed effects. I argue
that more capable students are more likely to adjust to online
instruction more effectively. Thus, I use a difference-in-difference
design and compared students with pre-course GPA above (high GPA) and
below (low GPA) the median before and during the pandemic. I observe
that during the pandemic, low-GPA students were less likely to answer
qualitatively similar questions from the pre-pandemic exams relative to
students with higher GPAs. My analysis of dynamic effects reveals that
by Spring 2022, the performance gap persisted between low and high GPA
students, both in overall exam scores and in their likelihood of
correctly answering nearly identical questions compared with
pre-pandemic levels.

I also analyze the students' performance on matched questions by
difficulty level. I find no statistically significant impact of the
pandemic on low GPA students' average probability of answering nearly
identical ``easy'' questions correctly, but I find significant effect on
their performance with nearly identical \emph{hard} questions. I also
provide a similar analysis comparing outcomes between students enrolled
in online and hybrid classes. My findings align with existing research
on learning loss during this period. By analyzing the performance of
qualitatively similar exam questions before and during the pandemic, I
contribute to the literature by offering more reliable estimates of
learning loss compared to traditional metrics, such as GPA and course
withdrawals.

The next section reviews the current literature on the effects of the
pandemic on college students' academic outcomes. Section 3 discusses the
data, section 4 explains the estimation strategy, section 5 reports the
results, and section 6 concludes the paper.

\section{Literature Review}\label{literature-review}

Most early studies analyzing the impact of COVID-19 on undergraduate
student outcomes were based on surveys about their experiences during
the pandemic. Jaeger et al. (\citeproc{ref-jaeger_global_2021}{2021})
was the first to document the negative impact of the COVID-19 pandemic
using surveys administered to university students in 28 universities in
the United States, Spain, Australia, Sweden, Austria, Italy, and Mexico
between April and October 2020. Their preliminary results reported
disparate impacts on different socio-economic and demographic groups.
Aucejo et al. (\citeproc{ref-aucejo_impact_2020}{2020}), one of the
first papers studying the effect of COVID-19 on college student
outcomes, surveyed 1,500 students at a large public university in the
United States. They found significant negative effects of the pandemic
on student outcomes. Due to the pandemic, 13\% of students delayed
graduation, 40\% lost a job, internship, or offer, and 29\% expected an
earnings loss by age 35. They also found large disparate impacts of the
pandemic across socio-economic statuses. Lower-income students were 55\%
more likely than their higher-income peers to have delayed graduation
due to COVID-19.

Along the same lines, Rodríguez-Planas
(\citeproc{ref-rodriguez-planas_hitting_2020}{2020}) collected data on
students' experiences during the pandemic using an online survey at an
urban public college in New York City in the summer of 2020. The author
found significant disruptions in students' lives due to the pandemic.
Because of COVID, between 14\% and 34\% of students considered dropping
a class during Spring 2020, 30\% modified their graduation plans, and
the freshman Fall retention rate dropped by 26\%. The pandemic also
deprived 39\% of students of their jobs, while 35\% of students saw
their earnings reduced. Pell grant recipients (students from
lower-income families) were 20\% more likely to lose a job due to the
pandemic and 17\% more likely to experience earning losses than non-Pell
recipients. Other vulnerable groups, such as first-generation and
transfer students, were relatively more affected. Since they seem to
rely less on financial aid and more on income from wage and salary jobs,
both their educational and employment outcomes were more negatively
impacted by the pandemic compared to students whose parents also
attended college or those who began college as freshmen.

The pandemic's impact on student learning was largely driven by the
sudden shift to remote instruction. Literature on remote learning shows
various approaches including fully remote, software-assisted, and hybrid
learning\footnote{see Escueta et al.
  (\citeproc{ref-escueta_education_2017}{2017}) for a comprehensive
  review.}. While online learning offers reduced costs in delivering
education and wider accessibility, research indicates mixed results.
Studies using randomized trials found that students in remote formats
generally performed worse than those in traditional settings Alpert,
Couch, and Harmon (\citeproc{ref-alpert_randomized_2016}{2016}).
Bettinger et al. (\citeproc{ref-bettinger_virtual_2017}{2017}) and
Cacault et al. (\citeproc{ref-cacault_distance_2021}{2021}) found that
online learning particularly disadvantaged lower-performing students.
Multiple analyses have demonstrated that online courses lead to lower
completion rates, grades, and persistence
(\citeproc{ref-jaggars_how_2016}{Jaggars and Xu 2016};
\citeproc{ref-xu_promises_2019}{Xu and Xu 2019}).

Several studies attempt to use the pandemic as an exogenous shock to
measure the impact of remote learning on college students' outcomes. For
instance, in their study, Altindag, Filiz, and Tekin
(\citeproc{ref-altindag_is_2021}{2021}) analyzed administrative data
from a public university and employed a fixed effects model. They
examine the effect of the change in learning modality due to the
pandemic on students' learning outcomes. They found that the online
instruction mode led to lower grades and an increased likelihood of
course withdrawal. Students who have had greater exposure to in-person
instruction have a lower likelihood of course repetition, a higher
probability of graduating on time, and achieving a higher graduation
GPA. Additionally, they observed that the difference in student
performance between in-person and online courses tended to diminish over
time in the post-pandemic era.

In the fall of 2020, Kofoed et al.
(\citeproc{ref-kofoed_zooming_2021}{2021}) randomized 551 West Point
students in a required introductory economics course across twelve
instructors into either an online or in-person class. They found that
final grades for online students dropped by 0.215 standard deviations.
This result was apparent in both assignments and exams and was largest
for academically at-risk students. Additionally, using a post-course
survey, they found that online students struggled to concentrate in
class and felt less connected to their instructors and peers. They
conclude that the shift to online education had negative effects on
learning. Using data on Virginia community college students, Bird,
Castleman, and Lohner (\citeproc{ref-bird_negative_2022}{2022}) applied
a difference-in-differences research design leveraging instructor fixed
effects and student fixed effects to estimate the impact of the
transition to online learning due to the pandemic. Their results show a
modest negative impact of 3\% - 6\% on course completion. Additionally,
their findings suggest that faculty experience in delivering online
lectures does not mitigate the negative effects. In their exploratory
analyses, they find minimal long-term effects of the switch to online
learning.

A comprehensive study by Bonacini, Gallo, and Patriarca
(\citeproc{ref-bonacini_unraveling_2023}{2023}), disentangle the
channels through which the pandemic affected students. They use admin
data from 2018-2021 of 36,000 university students in Italy who took
about 400,000 exams during this period. They examine the overall effect
of the pandemic on students' exam scores in different courses.
Additionally, they explore the effect of the transition to remote
learning by using COVID as an exogenous shock with a
difference-in-differences design. Their findings show that during the
pandemic, students performed better, with an increase in exam scores.
However, the abrupt move to remote learning decreased students' exam
scores.

Studies using survey data on students discussed above have found a
negative impact of COVID-related disruptions on academic performance.
However, studies that use measured outcomes to evaluate academic
performance report mixed results, especially immediately after the
pandemic began (\citeproc{ref-bird_negative_2022}{Bird, Castleman, and
Lohner 2022}; \citeproc{ref-bonacini_unraveling_2023}{Bonacini, Gallo,
and Patriarca 2023}). One reason for this might be that many
institutions temporarily implemented policies to reduce the burden on
students during the pandemic, particularly due to the sudden transition
from traditional to fully remote learning. Instructors were likely more
lenient in setting exam questions and grading, and more willing to
accommodate students than before the pandemic. The sudden move to remote
learning could have also created more opportunities for misbehavior by
students during exams. For instance, Rodríguez-Planas
(\citeproc{ref-rodriguez-planas_covid-19_2022}{2022}), using data from
an urban public college in NYC found that lower-income students were 35
percent more likely to utilize the flexible pass/fail grading policy.
While no GPA advantage is observed among top-performing lower-income
students, in the absence of the flexible grading policy these students
would have seen their GPA decrease by 5\% relative to their pre-pandemic
mean.

The literature has provided valuable insights into the impact of the
COVID-19 pandemic on undergraduates. However, several issues remain to
be addressed. Many studies rely on self-reported survey data, which may
not accurately capture the true extent of learning loss
(\citeproc{ref-aucejo_impact_2020}{Aucejo et al. 2020};
\citeproc{ref-rodriguez-planas_hitting_2020}{Rodríguez-Planas 2020}). I
identify major limitations in these recent studies. First, using course
completion rates, course GPAs, or end-of-semester GPAs to measure
academic outcomes immediately after COVID-19 hit in March may not
accurately reflect students' actual learning or learning loss. Second,
the pandemic-driven sudden transition to new instruction modalities
likely changed assessment methods as instructors and students took time
to adjust to the situation. The difficulty of exams immediately after
the adjustment may not have been the same as pre-COVID exams,
contributing to inaccurate measurement of learning loss. Additionally,
the implementation of flexible grading policies may have biased the
effect of the pandemic on course GPA or course completion rates. I
contribute to the literature in two ways. To address these limitations,
I analyze students' performance on common exams before and during the
pandemic. To eliminate variation due to changes in the difficulty of
exams during the pandemic, I examine students' performance on nearly
identical questions from exams before and during the pandemic to measure
learning loss.

\section{Data}\label{data}

The data for this study are drawn from two primary sources covering the
academic years 2019 through 2022. The first source records student
performance on common departmental final examinations for the
\emph{Introductory Microeconomics} course at a large public university
in New York City. This course is offered every semester, taught by
multiple instructors, and at least 700 students enroll annually.

The department offers the course through three modalities. \emph{Hybrid}
sections meet twice weekly, comprising one in-person session and one
fully remote session. \emph{Online} sections are conducted entirely
remotely using software. In 2019 (Spring and Fall), the course was
primarily offered in the hybrid mode, with one large online section.
Following the onset of the COVID-19 pandemic---spanning Fall 2020
through Spring 2022---offerings were exclusively online or hybrid, with
the exception of a single in-person section in 2022. I do not include
those students in the analyses to facilitate the comparison between the
efficacy of hybrid and online learning modes.

Although the course involves multiple instructors and teaching
modalities, assessment is standardized; all enrolled students are
required to complete a common, multiple-choice final examination with a
maximum score of 40 points. Leveraging performance on common exams
eliminates potential bias arising from heterogeneity in
instructor-specific testing difficulty. The dataset includes answer
sheets for all students who attempted these exams, providing the final
score, item-level performance, instructor identifiers, and the course
learning mode. Data for the Spring 2020 semester were unavailable.

The first measure is the aggregate score on the common final
examination, converted from a raw maximum of 40 points to a standard
0--100 scale. This standardized measure provides a more consistent
signal of learning than course GPA or withdrawal rates, which were
potentially confounded by flexible grading policies adopted during the
pandemic.

To exploit the granularity of the data, I also construct a second
measure based on item-level performance. Because the final exams are
departmental, I am able to match identical or nearly identical questions
appearing in exams administered both pre- and post-pandemic onset. While
the department utilizes two versions of the exam to deter cheating
(differing only in question order), the content remains constant. I
manually identified and matched 35 unique pairs of questions across the
pre-pandemic and pandemic periods. This allows for the construction of a
binary outcome variable, \emph{correct}, which takes a value of 1 if a
student answered the matched question correctly and 0 otherwise.

The second data source consists of institutional administrative records
for all students enrolled in \emph{Introductory Microeconomics} during
the relevant semesters. This dataset includes a rich set of covariates,
including gender, race, age, transfer status, enrollment intensity
(part-time vs.~full-time), native language, and class standing (freshman
through senior).

By merging these administrative records with the examination data, I
construct a comprehensive dataset linking student characteristics to
standardized performance metrics. This merger also incorporates
exam-level characteristics, such as learning modality, course
instructor, exam version, and the semester of administration. To my
knowledge, this is the first dataset to facilitate a granular
examination of COVID-19's impact on student performance using
standardized, item-level outcomes.

The final analytical sample comprises 4,655 unique students enrolled in
the course. For the granular analysis using matched exam questions, the
dataset expands to 47,589 student-question observations. Each
observation represents a student-question pair indicating whether the
specific item was answered correctly. For the majority of the sample, I
utilize the cumulative GPA recorded prior to the start of the semester.
If the pre-semester GPA is unavailable, I substitute it with the GPA
calculated at the end of the concurrent semester. In cases where both
values are missing, I impute the missing value using the mean GPA of the
student cohort for that specific semester.

\section{Estimation Strategy}\label{estimation-strategy}

To estimate the impact of the pandemic on student learning, I employ a
series of Ordinary Least Squares regressions. I first examine aggregate
performance on the common final examination, followed by an analysis of
item-level performance using matched questions across pre-pandemic and
pandemic semesters. My primary empirical strategy estimates the effect
of the pandemic on student outcomes using the following specification:

\begin{equation}
y_{i,c,t} = \delta P_{t} + \beta X_{i,c,t} + \gamma_{c} + \alpha_{s} + \epsilon_{i,c,t}
\end{equation}

\(y_{i,c,t}\) represents the academic outcome for student \(i\) in class
taught by instructor \(c\) during semester \(t\). I analyze two distinct
outcomes for \(y\). In the first set of regressions, \(y\) is the
students' aggregate score on the common final examination, scaled to a
0--100 range. In the second set of regressions, I employ a linear
probability model where \(y\) is a binary indicator equal to 1 if the
student answered a specific matched question correctly, and 0 otherwise.
For this item-level analysis, the subscript \(q\) is added to denote the
specific question pair, such that the outcome is denoted as
\(y_{i,c,q,t}\).

The variable of interest is \(P_{t}\), a binary indicator for the
pandemic period, equal to 1 for any semester after Fall 2019 (i.e.,
Spring 2020 onward) and 0 otherwise. The coefficient \(\delta\) captures
the average effect of the pandemic on student performance.

\(X_{i,c,t}\) includes student-level controls to account for demographic
and academic heterogeneity. These include dummy variables for race
(Black, Asian, non-White Hispanic, and Other, with White as the
reference group), gender (equal to 1 if female), and class standing
(equal to 1 if the student is a freshman or sophomore). To control for
baseline ability, I include the student's cumulative GPA prior to the
start of the course. The model also includes instructor fixed effects
(\(\gamma_{c}\)) and session fixed effects (\(\alpha_{s}\)) to control
for time-invariant instructor characteristics and semester-specific
shocks unrelated to the pandemic. Standard errors are robust to
heteroskedasticity.

\subsection{Identification of Differential Impact of COVID-19 on Low vs
High GPA
Students}\label{identification-of-differential-impact-of-covid-19-on-low-vs-high-gpa-students}

I further examine whether the pandemic differentially affected students
based on their prior academic performance. I classify students into
``Low GPA'' and ``High GPA'' groups based on the sample median
cumulative GPA of 3.32. Students below this threshold are defined as
low-GPA, while those at or above are defined as high-GPA. I estimate the
following interaction model:

\begin{equation}
y_{i,c,t} = \delta P_{t} + \phi L_{i} + \mu P_{t} \times L_{i} + \beta X_{i,c,t} + \gamma_{c} + \alpha_{s} + \epsilon_{i,c,t}
\end{equation}

Here, \(L_{i}\) is a binary indicator equal to 1 if student \(i\) is in
the low-GPA group. The coefficient of interest is \(\mu\), which
captures the differential impact of the pandemic on low-performing
students relative to their high-performing peers. A negative \(\mu\)
would indicate that the pandemic exacerbated inequality in learning
outcomes. This specification mirrors the baseline model but excludes
continuous cumulative GPA from the vector \(X_{i,c,t}\), as it is
captured by the group classification.

\subsection{Identification of the Effect of Sudden Transition to Remote
Learning}\label{identification-of-the-effect-of-sudden-transition-to-remote-learning}

Finally, I isolate the effect of the sudden, forced transition to remote
learning. While the aggregate pandemic effect (\(\delta\) in Equation 1)
captures broadly defined disruptions, a key mechanism was the shift in
instructional modality. Prior to the pandemic, the department offered
the course in two distinct modes: Hybrid (one in-person and one online
session weekly) and Online (fully remote). The onset of the pandemic in
March 2020 forced all hybrid sections to transition abruptly to a fully
remote format. I exploit this variation using a
difference-in-differences framework to estimate the impact of
transitioning from hybrid to remote learning, relative to students who
were already enrolled in fully online sections:

\begin{equation}
y_{i,c,t} = \delta P_{t} + \phi O_{i} + \mu P_{t} \times O_{i} + \beta X_{i,c,t} + \gamma_{c} + \alpha_{s} + \epsilon_{i,c,t}
\end{equation}

In this specification, \(O_{i}\) is an indicator equal to 1 if the
student originally enrolled in an online section and 0 if they enrolled
in a hybrid section. The interaction term \(\mu\) tests whether students
who selected into online learning---and thus experienced less disruption
in modality---performed differently during the pandemic compared to
those forced to switch from hybrid to remote instruction. Control
variables remain consistent with the baseline specification, excluding
the instruction mode indicator which is subsumed by the
difference-in-differences terms.

\section{Results}\label{results}

\subsection{Average Course GPA Across
Semesters}\label{average-course-gpa-across-semesters}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{working_paper_2_files/figure-pdf/gpa-across-semesters-eco-1001-1.pdf}}

}

\caption{Average Course GPA across semesters}

\end{figure}%

A central premise of this study is that conventional metrics commonly
cited in the literature---such as course completion rates, withdrawal
rates, and GPAs---may fail to accurately capture academic performance
during the pandemic period. The abrupt transition in instructional
modality likely induced behavioral responses from instructors, including
increased leniency in grading standards to accommodate the disruptive
environment. Furthermore, the shift to remote assessment may have
introduced opportunities for academic misconduct, thereby inflating
scores. Consequently, relying solely on GPA may mask the true negative
impact of the pandemic on human capital accumulation, as these
institutional policy shifts and grading accommodations can overshadow
actual declines in content mastery.

Figure 1 presents the temporal evolution of unadjusted average GPAs in
the introductory microeconomics course. The data reveal a sharp
discontinuity in Spring 2020, coinciding with the transition to online
instruction. This increase in average grades contradicts contemporaneous
survey evidence suggesting that students faced substantial personal and
academic hardships during this period
(\citeproc{ref-aucejo_impact_2020}{Aucejo et al. 2020};
\citeproc{ref-rodriguez-planas_hitting_2020}{Rodríguez-Planas 2020}). If
GPAs were a consistent proxy for learning, one would expect a decline in
grades reflecting these documented struggles. Instead, while GPAs
moderated slightly in Fall 2020 and Spring 2021, they remained
persistently elevated above pre-pandemic levels until after Fall 2021,
suggesting a structural break in grading norms rather than an
improvement in student performance.

\subsection{Withdrawal Rates and Institutional Grading
Policies}\label{withdrawal-rates-and-institutional-grading-policies}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{working_paper_2_files/figure-pdf/withdrawal-rate-1.pdf}}

}

\caption{Withdrawal rates in ECO 1001 across semesters}

\end{figure}%

A second factor complicating the use of traditional performance metrics
is the implementation of flexible grading policies. Mirroring trends
across the US higher education system, the institution under study
adopted temporary measures to mitigate the academic burden of the
pandemic. Through the final day of the semester, students were allowed
to elect one of three options: a ``Credit'' (CR) designation, which
conferred course credit without impacting GPA; a ``No Credit'' (NC)
designation, which allowed for course completion without credit or a
recorded withdrawal; or the standard course withdrawal.

Figure 2 illustrates the unadjusted rates of withdrawal and non-standard
grade designations across the sample period. In Spring 2020, the onset
of the pandemic coincided with an atypically low standard withdrawal
rate of 3.92\%. This decline was offset by substantial utilization of
the flexible grading options: 29.75\% of enrolled students elected the
CR designation, while 5.53\% opted for NC. Following the discontinuation
of this policy after Spring 2020, standard withdrawal rates rebounded,
rising to 6.65\% in Fall 2020 and approaching 8\% by Spring 2022.
Because these policy shifts fundamentally altered the incentives driving
course retention and GPA calculation, reliance on completion rates or
grade averages fails to provide a consistent counterfactual for
assessing the pandemic's true impact on learning.

\subsection{Descriptive Statistics}\label{descriptive-statistics}

\begin{table}[H]
\centering
\caption{\label{tab:descriptive-stats}Descriptive Statistics}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\fontsize{9}{11}\selectfont
\begin{threeparttable}
\begin{tabular}[t]{lllll}
\toprule
\multicolumn{1}{c}{\em{\textbf{ }}} & \multicolumn{1}{c}{\em{\textbf{Pre-COVID (N = 752)}}} & \multicolumn{1}{c}{\em{\textbf{Post-COVID (N = 3846)}}} & \multicolumn{2}{c}{\em{\textbf{ }}} \\
\cmidrule(l{3pt}r{3pt}){2-2} \cmidrule(l{3pt}r{3pt}){3-3}
  & Mean & Mean & Diff. in Means & Std.error\\
\midrule
Final exam score & 56.586 & 57.148 & 0.562 & 0.606\\
Correct & 0.620 & 0.582 & -0.038*** & 0.010\\
Hispanic & 0.133 & 0.189 & 0.056*** & 0.014\\
Black & 0.082 & 0.077 & -0.005 & 0.011\\
Asian & 0.512 & 0.457 & -0.055** & 0.020\\
\addlinespace
Other race & 0.012 & 0.060 & 0.048*** & 0.006\\
Fall & 0.480 & 0.542 & 0.062** & 0.020\\
Online & 0.346 & 0.585 & 0.239*** & 0.019\\
GPA & 3.146 & 3.302 & 0.155*** & 0.035\\
Female & 0.440 & 0.464 & 0.024 & 0.020\\
\addlinespace
Age & 21.352 & 20.219 & -1.133*** & 0.178\\
Parttime & 0.082 & 0.051 & -0.031** & 0.011\\
Sophomore or below & 0.840 & 0.935 & 0.094*** & 0.014\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para]
\item \textit{Note: } 
\item * p < 0.1, ** p < 0.05, *** p < 0.01. Final exam scores are based on a 100-point scale.
\end{tablenotes}
\end{threeparttable}}
\end{table}

Table 1 summarizes the sample characteristics across the pre-pandemic
(Spring and Fall 2019) and pandemic (Fall 2020 through Spring 2022)
periods. The final analytical sample consists of 4,598 students, with
752 observed in the pre-pandemic period and 3,846 in the pandemic
period.

In terms of aggregate academic outcomes, the unadjusted difference in
final exam scores is negligible (0.562 points) and statistically
insignificant. However, granular performance on matched item-level
questions reveals a statistically significant decline; the unadjusted
probability of answering a matched question correctly is 3.8 percentage
points lower in the post-pandemic period compared to the pre-pandemic
baseline.

The demographic composition of the student body shifted significantly
between the two periods. The share of Hispanic students increased from
13.3\% to 18.9\%, while the share of Asian students declined by 5.5
percentage points. Black enrollment remained stable, with no
statistically significant change. Enrollment modality also saw a
structural shift: prior to the pandemic, approximately 35\% of students
selected fully online sections. In the post-pandemic period, this figure
rose to nearly 59\%, driven by the exclusive use of remote instruction
in Fall 2020 and Spring 2021, and a continued preference for online
options when hybrid sections resumed in Spring 2022.

The post-pandemic cohort is also distinct in terms of age and academic
maturity. Students in the pandemic period are, on average, roughly one
year younger (\(p < 0.01\)) and are more likely to be freshmen or
sophomores (an increase of approximately 9.4 percentage points).
Proportion of part-time students also decreased by about 3 percentage
points.

Finally, Table 1 reports the baseline cumulative GPA, a critical control
for student ability. As noted in the data section, missing baseline GPA
values are imputed using the end-of-semester GPA or, if both are
unavailable, the semester-specific mean GPA.

\subsection{Baseline Specification}\label{baseline-specification}

\begin{table}[H]
\centering
\caption{\label{tab:base-specification}Baseline estimates of effects of COVID-19 on student performance}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\fontsize{9}{11}\selectfont
\begin{threeparttable}
\begin{tabular}[t]{lcccc}
\toprule
\multicolumn{1}{c}{\em{\textbf{ }}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Final Exam Score\\(mean = 57.1, sd = 15.6)}}}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Did Student Get The Answer Correct (Y/N)?\\(mean = 0.6, sd = 0.49)}}}} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
  & (1) & (2) & (3) & (4)\\
\midrule
postcovid & -1.151 &  & -0.019*** & \\
 & (0.744) &  & (0.007) & \\
Fall 2020 &  & 1.308 &  & -0.102***\\
 &  & (1.504) &  & (0.013)\\
Spring 2021 &  & -5.760*** &  & -0.082***\\
\addlinespace
 &  & (1.135) &  & (0.015)\\
Fall 2021 &  & 5.643*** &  & -0.019**\\
 &  & (1.169) &  & (0.010)\\
Spring 2022 &  & -6.954*** &  & -0.088***\\
 &  & (1.370) &  & (0.012)\\
\addlinespace
Num.Obs. & 4598 & 4598 & 47589 & 47589\\
R2 & 0.209 & 0.223 & 0.036 & 0.039\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para]
\item \textit{Note: } 
\item * p < 0.1, ** p < 0.05, *** p < 0.01. Final exam scores are based on a 100-point scale. Heteroskedasticity-robust standard errors are used. All regressions include the following control variables: cumulative GPA, gender, race, age, whether a student is at least a sophomore, part-time status of the student. All regressions also include a dummy variable, gpamiss, which is 1 if cumulative GPA is imputed using the mean and 0 otherwise. All regressions include course instructor fixed-effects and session fixed-effects.
\end{tablenotes}
\end{threeparttable}}
\end{table}

Table 2 presents the results from the baseline OLS specifications
estimating the impact of the pandemic on student performance. Columns
(1) and (2) report results where the dependent variable is the aggregate
final exam score (0--100 scale), while Columns (3) and (4) report
estimates from linear probability models where the outcome is binary (1
if a matched question was answered correctly, 0 otherwise). All
specifications control for student demographics (race, gender, age),
academic background (cumulative GPA, class standing), enrollment status
(part-time), and include instructor and session fixed effects. To
account for potential bias from missing data, I also include a dummy
variable, \emph{gpamiss}, indicating observations where cumulative GPA
was imputed; inclusion of this control does not qualitatively alter the
results.

Column (1) estimates the average effect of the pandemic across all
post-2019 semesters. The coefficient on the pandemic indicator is
negative (\(-1.151\)), suggesting a decline in average scores by
approximately 1.15 points, though this estimate is not statistically
significant.

Column (2) disaggregates this effect by semester, using the pooled 2019
academic year as the reference category. The results reveal substantial
temporal heterogeneity. In Fall 2020, scores were statistically
indistinguishable from pre-pandemic levels (\(1.308\), \(p > 0.10\)).
However, performance dropped sharply in Spring 2021, with scores
declining by 5.76 points (\(p < 0.01\)), equivalent to a 0.37 standard
deviation decrease. While performance rebounded temporarily in Fall 2021
(\(+5.64\) points), it fell again in Spring 2022 by 6.95 points
(\(p < 0.01\)).

Columns (3) and (4) leverage the granular, item-level data to examine
performance on identical or nearly identical questions. Column (3) shows
that, on average across the entire post-pandemic period, the probability
of answering a matched question correctly decreased by 1.9 percentage
points (\(p < 0.01\)) relative to the pre-pandemic baseline.

Column (4) illustrates that this learning loss was immediate and
persistent. In Fall 2020---despite aggregate exam scores remaining
stable---the probability of answering a matched question correctly
plummeted by 10.2 percentage points (\(p < 0.01\)) relative to 2019.
This negative effect persisted throughout the observation period, with
the probability of a correct response remaining 8.8 percentage points
lower (\(p < 0.01\)) in Spring 2022 compared to the pre-pandemic
reference.

\subsection{Differential Impact Across GPA
Quartiles}\label{differential-impact-across-gpa-quartiles}

\begin{table}[H]
\centering
\caption{\label{tab:gpa-by-quartiles}Differential effects of COVID-19 across GPA quartiles}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\fontsize{9}{11}\selectfont
\begin{threeparttable}
\begin{tabular}[t]{lcccc}
\toprule
\multicolumn{1}{c}{\em{\textbf{ }}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Final Exam Score\\(mean = 57.1, sd = 15.6)}}}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Did Student Get The Answer Correct (Y/N)?\\(mean = 0.6, sd = 0.49)}}}} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
  & (1) & (2) & (3) & (4)\\
\midrule
postcovid & -1.894** & 1.564 & -0.022*** & 0.002\\
 & (0.739) & (1.421) & (0.007) & (0.010)\\
GPA (first quartile) & -18.335*** & -13.299*** & -0.202*** & -0.176***\\
 & (0.610) & (1.673) & (0.006) & (0.010)\\
GPA (second quartile) & -15.034*** & -11.586*** & -0.146*** & -0.116***\\
\addlinespace
 & (0.608) & (1.566) & (0.007) & (0.011)\\
GPA (third quartile) & -10.191*** & -8.107*** & -0.112*** & -0.119***\\
 & (0.573) & (1.958) & (0.006) & (0.011)\\
post x GPA (first quartile) &  & -5.722*** &  & -0.041***\\
 &  & (1.784) &  & (0.013)\\
\addlinespace
post x GPA (second quartile) &  & -3.946** &  & -0.059***\\
 &  & (1.699) &  & (0.014)\\
post x GPA (third quartile) &  & -2.362 &  & 0.008\\
 &  & (2.048) &  & (0.013)\\
Num.Obs. & 4598 & 4598 & 47589 & 47589\\
\addlinespace
R2 & 0.246 & 0.248 & 0.040 & 0.040\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para]
\item \textit{Note: } 
\item * p < 0.1, ** p < 0.05, *** p < 0.01. Final exam scores are based on a 100-point scale. Heteroskedasticity-robust standard errors are used. All regressions include the following control variables: cumulative GPA, gender, race, age, whether a student is at most a sophomore, part-time status of the student. All regressions also include a dummy variable, gpamiss, which is 1 if cumulative GPA is imputed using the mean and 0 otherwise. All regressions include course instructor fixed-effects and session fixed-effects.
\end{tablenotes}
\end{threeparttable}}
\end{table}

To investigate distributional impacts, I estimate the differential
effect of the pandemic across students' baseline academic performance. I
stratify the sample into quartiles based on cumulative GPA. For the
aggregate exam score analysis, the quartiles are defined as: Q1 (GPA
\(\leq\) 3.01), Q2 (3.01 \textless{} GPA \(\leq\) 3.37), Q3 (3.37
\textless{} GPA \(\leq\) 3.71), and Q4 (GPA \textgreater{} 3.71).
Similarly, for the matched-question analysis, the thresholds are: Q1
(GPA \(\leq\) 3.08), Q2 (3.08 \textless{} GPA \(\leq\) 3.32), Q3 (3.32
\textless{} GPA \(\leq\) 3.68), and Q4 (GPA \textgreater{} 3.68). Using
the top quartile (Q4) as the reference group, I examine whether the
pandemic exacerbated achievement gaps for lower-performing students.

Table 3 presents the results. Column (1) confirms the expected baseline
achievement gap: students in the bottom quartile score, on average, 18.3
points lower than their top-quartile peers. Column (2) reveals that the
pandemic significantly widened this gap. The interaction term for the
bottom quartile (\texttt{post\ x\ GPA\ (first\ quartile)}) is negative
and statistically significant (\(-5.722\), \(p < 0.01\)), indicating
that the achievement gap between the highest and lowest performing
students expanded by an additional 5.7 points during the pandemic. A
similar, though smaller, widening of the gap is observed for the second
quartile (\(-3.946\), \(p < 0.05\)), while the third quartile shows no
statistically significant differential impact relative to the top
quartile.

Columns (3) and (4) report results for the item-level analysis. The
baseline probability of answering a matched question correctly is 20.2
percentage points lower for bottom-quartile students compared to
top-quartile students (Column 3). Column (4) shows that the pandemic
exacerbated this disparity: the interaction term indicates that the
probability of a correct response for bottom-quartile students fell by
an additional 4.1 percentage points (\(p < 0.01\)) relative to the top
quartile. The second quartile also experienced a significant relative
decline of 5.9 percentage points (\(p < 0.01\)). These findings provide
robust evidence that the pandemic disproportionately harmed students
with weaker academic backgrounds, significantly exacerbating
pre-existing inequalities in learning outcomes.

\subsection{Impact of COVID on Low GPA
Students}\label{impact-of-covid-on-low-gpa-students}

Building on the quartile-based heterogeneity patterns documented above,
I next assess whether the pandemic disproportionately affected students
with weaker prior academic performance using a median split. Panel A of
Table 4 reports estimates comparing students in the low-GPA group
(defined as cumulative GPA below the sample median) to their high-GPA
counterparts.

Columns (1) and (2) use final exam scores as the outcome. Consistent
with substantial pre-existing achievement gaps, low-GPA students score
11.37 points lower than high-GPA students on the common final exam
(Column 1). Column (2) adds an interaction between the low-GPA indicator
and the post-pandemic period indicator, yielding a
difference-in-differences estimate of the additional pandemic-period
change for low-GPA students relative to the high-GPA reference group.
The interaction term is negative and statistically significant: low-GPA
students experienced an additional 3.25-point decline in exam scores
during the post-pandemic period compared to high-GPA students.

Columns (3) and (4) report analogous linear probability models using
matched question performance. Column (3) indicates that, on average,
low-GPA students are 13.2 percentage points less likely to answer a
matched question correctly than high-GPA students. Column (4) shows that
this gap widened further during the pandemic: the interaction term
implies an additional 3.3 percentage point reduction in the probability
of answering a matched question correctly for low-GPA students in the
post-pandemic period (p \textless{} 0.01). Taken together, these
estimates provide consistent evidence that the pandemic exacerbated
learning losses among academically vulnerable students, particularly
when performance is measured using standardized, item-level outcomes.

\begin{table}[H]
\centering\begingroup\fontsize{9}{11}\selectfont

\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{threeparttable}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}cccc}
\toprule
\multicolumn{1}{c}{\em{\textbf{ }}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Final Exam Score \\ (mean = 57.1, sd = 15.6)}}}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Did Student Get The Answer Correct (Y/N)? \\ (mean = 0.6, sd = 0.49)}}}} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
  & (1) & (2) & (3) & (4)\\
\midrule
\addlinespace[0.3em]
\multicolumn{5}{l}{\textit{\textbf{Panel A: Low vs High GPA}}}\\
\hspace{1em}postcovid & -2.637*** & -0.477 & -0.014* & 0.001\\
\hspace{1em} & (0.750) & (1.139) & (0.007) & (0.008)\\
\hspace{1em}lowgpa & -11.369*** & -8.541*** & -0.132*** & -0.113***\\
\hspace{1em} & (0.442) & (1.225) & (0.005) & (0.008)\\
\hspace{1em}post x lowgpa &  & -3.245** &  & -0.033***\\
\hspace{1em} &  & (1.303) &  & (0.010)\\
\hspace{1em}Num.Obs. & 4598 & 4598 & 47589 & \vphantom{1} 47589\\
\hspace{1em}R2 & 0.187 & 0.188 & 0.034 & 0.035\\
\addlinespace[0.3em]
\multicolumn{5}{l}{\textit{\textbf{Panel B: Online vs Hybrid}}}\\
\hspace{1em}postcovid & -1.151 & 0.971 & -0.019*** & -0.040***\\
\hspace{1em} & (0.744) & (0.962) & (0.007) & (0.009)\\
\hspace{1em}online & -1.778*** & 2.987** & -0.076*** & -0.106***\\
\hspace{1em} & (0.592) & (1.408) & (0.007) & (0.010)\\
\hspace{1em}post x online &  & -5.262*** &  & 0.060***\\
\hspace{1em} &  & (1.428) &  & (0.014)\\
\hspace{1em}Num.Obs. & 4598 & 4598 & 47589 & 47589\\
\hspace{1em}R2 & 0.209 & 0.211 & 0.036 & 0.037\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para]
\item \textit{Note: } 
\item * p < 0.1, ** p < 0.05, *** p < 0.01. Final exam scores are based on a 100-point scale. Heteroskedasticity-robust standard errors are used. All regressions include the following control variables: cumulative GPA, gender, race, age, whether a student is at most a sophomore, part-time status of the student. All regressions also include a dummy variable, gpamiss, which is 1 if cumulative GPA is imputed using the mean and 0 otherwise. All regressions include course instructor fixed-effects and session fixed-effects.
\end{tablenotes}
\end{threeparttable}}
\endgroup{}
\end{table}

\subsection{Abrupt Transition to Remote
Learning}\label{abrupt-transition-to-remote-learning}

To isolate the role of modality disruption from broader pandemic-era
shocks, Panel B of Table 4 reports difference-in-differences estimates
comparing students enrolled in fully online sections to those enrolled
in hybrid sections prior to the pandemic. This design leverages the fact
that, before COVID-19, the department offered both modalities, while the
onset of the pandemic triggered an abrupt shift to fully remote
instruction for all sections.

Columns (1) and (2) present OLS estimates with final exam scores as the
outcome. Conditional on the post-pandemic indicator, students enrolled
in online sections score 1.78 points lower than those enrolled in hybrid
sections (Column 1). Column (2) adds an interaction between the
post-pandemic indicator and the online-modality indicator. The
interaction term captures the additional change in performance for
students initially enrolled in online sections---relative to those
initially enrolled in hybrid sections---after instruction moved fully
remote. The estimate is negative and statistically significant: the
interaction coefficient is \(-5.262\), indicating a sizable decline in
exam scores associated with the pandemic-induced transition to remote
learning.

Columns (3) and (4) report analogous linear probability models using
matched question performance. Column (3) shows that, on average,
students in online sections are 7.6 percentage points less likely to
answer a matched question correctly than students in hybrid sections,
conditional on the pandemic indicator. In Column (4), the
difference-in-differences interaction term is positive and statistically
significant (\(0.060\), \(p < 0.01\)), implying that the relative
probability of answering a matched question correctly increased by 6.0
percentage points for students in online sections after the onset of the
pandemic. While this pattern contrasts with the negative interaction
estimates for aggregate exam scores, it suggests that the mechanisms
linking instructional modality to performance may differ across outcome
measures, potentially reflecting differences in assessment conditions or
adaptation over time.

All specifications include the same set of student covariates as in the
baseline models (cumulative GPA, gender, race, age, class standing, and
part-time status), as well as the GPA-imputation indicator
\emph{gpamiss}. Instructor and session fixed effects are included
throughout, and standard errors are heteroskedasticity-robust.

\subsection{Dynamic Effects}\label{dynamic-effects}

The preceding analyses document substantial average effects and
heterogeneity in pandemic-era learning losses. I next examine whether
these disparities persisted, attenuated, or intensified over time by
estimating semester-specific effects. Specifically, I interact the
low-GPA indicator with a full set of semester dummies, using the pooled
2019 cohorts (Spring and Fall 2019) as the reference period. This
event-study style specification traces the evolution of outcome gaps
between low- and high-GPA students across post-2019 semesters. I
implement an analogous exercise for instructional modality by
interacting the online-enrollment indicator with the same semester
dummies, again using the pooled 2019 cohorts as the benchmark.

Figure 3 summarizes these dynamic patterns for both outcome measures.
The top-left panel (final exam scores: low vs.~high GPA) shows a
pronounced deterioration for low-GPA students immediately following the
onset of the pandemic, with the largest negative effect occurring in
Fall 2020. Although the estimates partially recover in later semesters,
the gap relative to high-GPA students remains negative throughout the
observation window, indicating persistent learning losses for
academically vulnerable students.

The top-right panel (final exam scores: online vs.~hybrid) highlights
the temporal profile of the modality shock. The transition to fully
remote instruction is associated with an initial decline in exam scores,
followed by improvement by the subsequent semester, consistent with
gradual adaptation by students and instructors to the remote learning
environment.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{working_paper_2_files/figure-pdf/dynamic-effects-1.pdf}

}

\caption{Dynamic effects of COVID-19 on students' performance}

\end{figure}%

A similar---but in important respects more persistent---pattern emerges
when performance is measured using matched-question outcomes. The
bottom-left panel (matched questions: low vs.~high GPA) indicates a
sharp drop in the probability that low-GPA students answer a matched
question correctly in the early pandemic period, with little evidence of
convergence back to pre-pandemic differentials through Spring 2022. By
contrast, the bottom-right panel (matched questions: online vs.~hybrid)
suggests that the initial decline associated with the shift to online
instruction attenuates over time, with estimates moving back toward
pre-pandemic levels by the later period. Taken together, these results
underscore that conclusions about recovery depend on the outcome metric:
aggregate exam scores suggest partial rebound, whereas item-level
performance points to more persistent deficits for low-GPA students.

\subsection{Effects Due to Heterogenity in Difficulty of
Questions}\label{effects-due-to-heterogenity-in-difficulty-of-questions}

The preceding results indicate that pandemic-era learning losses are
concentrated among academically vulnerable students and vary by
instructional modality. I next examine whether these patterns differ
systematically by item difficulty. Because all ECO 1001 students take a
common final exam, and exam items were pre-classified by instructors as
either \emph{easy} or \emph{hard}, I can stratify the matched-question
analysis by difficulty while holding item content constant. Importantly,
I retain the original (pre-pandemic) difficulty designation for each
matched item---an item is treated as \emph{hard} (or \emph{easy}) if it
was labeled as such in the pre-pandemic exams---thereby minimizing
concerns that observed performance changes reflect shifts in exam
composition rather than changes in mastery.

Table 5 reports the resulting estimates. Panel A compares low- and
high-GPA students. For hard questions, low-GPA students are
substantially less likely to answer correctly than high-GPA students (a
14.1 percentage point gap in Column 3). Moreover, this disparity widens
in the post-pandemic period: the interaction term in Column 4 indicates
an additional 3.7 percentage point decline for low-GPA students on hard
items relative to high-GPA students (\(p < 0.01\)). For easy questions,
low-GPA students also perform worse on average (about 9--10 percentage
points), but the post-pandemic interaction is small and statistically
indistinguishable from zero, suggesting that the additional pandemic-era
decline for low-GPA students is concentrated in more difficult content.

\begin{table}[H]
\centering\begingroup\fontsize{9}{11}\selectfont

\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{threeparttable}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}cccc}
\toprule
\multicolumn{1}{c}{\em{\textbf{ }}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Easy Questions \\ (mean = 0.645, sd = 0.479)}}}} & \multicolumn{2}{c}{\em{\textbf{\makecell[c]{Hard Questions \\ (mean = 0.573, sd = 0.495)}}}} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
  & (1) & (2) & (3) & (4)\\
\midrule
\addlinespace[0.3em]
\multicolumn{5}{l}{\textit{\textbf{Panel A: Low vs High GPA}}}\\
\hspace{1em}postcovid & -0.006 & -0.018 & -0.003 & 0.014\\
\hspace{1em} & (0.018) & (0.020) & (0.010) & (0.011)\\
\hspace{1em}lowgpa & -0.092*** & -0.101*** & -0.141*** & -0.120***\\
\hspace{1em} & (0.009) & (0.013) & (0.006) & (0.010)\\
\hspace{1em}post x lowgpa &  & 0.022 &  & -0.037***\\
\hspace{1em} &  & (0.019) &  & (0.013)\\
\hspace{1em}Num.Obs. & 13332 & 13332 & 28018 & \vphantom{1} 28018\\
\hspace{1em}R2 & 0.033 & 0.033 & 0.037 & 0.037\\
\addlinespace[0.3em]
\multicolumn{5}{l}{\textit{\textbf{Panel B: Online vs Hybrid}}}\\
\hspace{1em}postcovid & 0.003 & -0.005 & -0.009 & -0.044***\\
\hspace{1em} & (0.018) & (0.021) & (0.010) & (0.012)\\
\hspace{1em}online & -0.126*** & -0.135*** & -0.047*** & -0.093***\\
\hspace{1em} & (0.017) & (0.022) & (0.010) & (0.013)\\
\hspace{1em}post x online &  & 0.019 &  & 0.100***\\
\hspace{1em} &  & (0.027) &  & (0.018)\\
\hspace{1em}Num.Obs. & 13332 & 13332 & 28018 & 28018\\
\hspace{1em}R2 & 0.031 & 0.031 & 0.042 & 0.043\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para]
\item \textit{Note: } 
\item * p < 0.1, ** p < 0.05, *** p < 0.01. Heteroskedasticity-robust standard errors are used. All regressions include the following control variables: cumulative GPA, gender, race, and part-time status of the student. All regressions also include a dummy variable, gpamiss, which is 1 if cumulative
    GPA is imputed using the mean and 0 otherwise. All regressions include session fixed-effects and course instructor fixed-effects.
\end{tablenotes}
\end{threeparttable}}
\endgroup{}
\end{table}

Panel B conducts an analogous analysis comparing online versus hybrid
instruction. Students enrolled in online sections perform worse on both
easy and hard items in the pre-pandemic baseline period (Columns 1 and
3). However, following the pandemic-induced shift to remote learning,
the interaction term for online enrollment is positive for hard
questions (Column 4: \(0.100\), \(p < 0.01\)), indicating a relative
improvement in hard-item performance for students associated with the
online modality during the post-pandemic period. By contrast, the
corresponding interaction for easy questions is not statistically
significant (Column 2), implying that any relative gains are
concentrated among hard items.

Overall, stratifying by difficulty suggests that the pandemic
disproportionately reduced performance on harder material for low-GPA
students, while the modality-related dynamics differ by outcome type and
are most pronounced for hard questions.

\section{Conclusion}\label{conclusion}

In this essay, I examine the pandemic's influence on the academic
performance of students by analyzing their results in the common exams
for introductory microeconomics course at a large public university in
New York City. I advance the literature by providing estimates of
learning loss in college students due to pandemic that are more reliable
than current estimates. I use two outcome measures to evaluate students'
academic performance and argue that these outcome choices are more
appropriate than the existing outcome measures such as course completion
rate, course GPA, or semester GPA used in the literature on the impact
of COVID on students' academic performance. First, I analyze students'
scores on common final exams administered at the institution from 2019
to 2022, excluding Spring 2020 due to lack of data availability for that
semester. Acknowledging the fact that difficulty of exams may have
changed during the pandemic, I use 35 pairs of questions matched from
these common final exams to measure changes in the students' average
probability of answering nearly identical questions from the exams
conducted before and during the pandemic to eliminate the variation from
exam difficulty. I find an overall negative impact of the pandemic on
students' outcomes. Students' scores went down by a point in the full
pandemic period (2020-2022), although the coefficient is not
statistically significant. Students' average probability of answering
similar questions from the common exams before the pandemic went down
during the pandemic by 1.5 percentage points. This clear evidence of
learning loss, I argue, is not affected by the flexible grading policy.
The extent of learning loss was greater in Fall 2020 and gradually
lessened through Fall 2021, after which it stabilized.

I also examine the differential impact of the pandemic on the outcomes
of students with low GPA compared to those with high GPA. My findings
suggest that on average low GPA students have a 3.3 percentage point
lower average probability of correctly answering similar questions
compared to high GPA students during the pandemic. This accounts for a
broad range of student characteristics and incorporates instructor and
session fixed effects, indicating a significant differential impact on
low GPA students. While using students' scores from common exams as the
outcome variable, I find that low GPA students on average scored 3.23
points less in the common exams compared to high GPA students during the
pandemic. In the long term, although this difference decreases, it does
not return to the pre-pandemic level by Spring 2022. Additionally, I
examined the pandemic's effects across GPA quartiles and found that
students in the lowest quartile of GPA distribution were 4.1 percentage
points less likely to correctly answer nearly identical questions from
pre-pandemic exams during the pandemic. This analysis supports the
hypothesis that low GPA students, on average, suffered greater learning
loss due to the pandemic compared to high GPA students.

Furthermore, I explore an important channel: the sudden shift to online
classes, through which the pandemic affected students' academic
outcomes. I find that abruptly moving to online classes due to the
pandemic reduced students' final exam scores by 5.43 points. In case of
matched questions data, the mean probability of answering a similar
question before and after suddenly moving to online classes increased by
5.6 percentage points. Interacting the semester dummies with a dummy for
online variable, I find that the abrupt transition to online classes
reduced the average probability of answering a similar question
correctly before and during pandemic before returning to pre-pandemic
levels. The same pattern is observed in case of exam scores as outcome
variable. To examine how sensitive these estimates of learning loss are
to question difficulty in the matched questions data, I provide results
from separate analyses using easy as well as hard questions. During the
pandemic, low-GPA students' mean probability of answering nearly
identical hard questions decreased by 4.5 percentage points relative to
their high-GPA counterparts. I found no statistically significant effect
for easy questions. When examining the effect of abrupt transition to
remote classes, I found that students scored just over 12 percentage
points higher on hard questions after moving online, while showing no
statistically significant difference on easy questions.

Overall, I find negative effects of the pandemic on students' academic
performance that align directionally with the current literature. My
unique matched questions data allows me to eliminate bias in the
estimates that arose from flexible grading policies implemented
immediately after the pandemic hit educational institutions nationwide.
I do, however, acknowledge that my estimates may not account fully for
potential cheating by students, especially in the initial months
following a sudden transition to remote classes. The implications of
learning loss due to the pandemic could be significant. On one hand,
students' GPAs, both course-specific and overall, did not change much or
even increased in some cases during the pandemic, giving the impression
of better performance. On the other hand, evidence from student surveys
shows that students faced hardships and challenges in learning during
this time. In my study I provide evidence of learning loss which is
consistent with students' negative experiences during the pandemic. In
future, any decision to suddenly switch to remote learning during a
complex situation should be carefully considered before implementation.

\pagebreak

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-alpert_randomized_2016}
Alpert, William T., Kenneth A. Couch, and Oskar R. Harmon. 2016. {``A
{Randomized} {Assessment} of {Online} {Learning}.''} \emph{American
Economic Review} 106 (5): 378--82.
\url{https://doi.org/10.1257/aer.p20161057}.

\bibitem[\citeproctext]{ref-altindag_is_2021}
Altindag, Duha Tore, Elif S. Filiz, and Erdal Tekin. 2021. {``Is
{Online} {Education} {Working}?''} Working \{Paper\}. Working {Paper}
{Series}. National Bureau of Economic Research.
\url{https://doi.org/10.3386/w29113}.

\bibitem[\citeproctext]{ref-aucejo_impact_2020}
Aucejo, Esteban M., Jacob French, Maria Paola Ugalde Araya, and Basit
Zafar. 2020. {``The Impact of {COVID}-19 on Student Experiences and
Expectations: {Evidence} from a Survey.''} \emph{Journal of Public
Economics} 191 (November): 104271.
\url{https://doi.org/10.1016/j.jpubeco.2020.104271}.

\bibitem[\citeproctext]{ref-bettinger_virtual_2017}
Bettinger, Eric P., Lindsay Fox, Susanna Loeb, and Eric S. Taylor. 2017.
{``Virtual {Classrooms}: {How} {Online} {College} {Courses} {Affect}
{Student} {Success}.''} \emph{American Economic Review} 107 (9):
2855--75. \url{https://doi.org/10.1257/aer.20151193}.

\bibitem[\citeproctext]{ref-bird_negative_2022}
Bird, Kelli A., Benjamin L. Castleman, and Gabrielle Lohner. 2022.
{``Negative {Impacts} from the {Shift} to {Online} {Learning} During the
{COVID}-19 {Crisis}: {Evidence} from a {Statewide} {Community} {College}
{System}.''} \emph{AERA Open} 8 (1).
\url{https://doi.org/10.1177/23328584221081220}.

\bibitem[\citeproctext]{ref-bonacini_unraveling_2023}
Bonacini, Luca, Giovanni Gallo, and Fabrizio Patriarca. 2023.
{``Unraveling the Controversial Effect of {Covid}-19 on College
Students' Performance.''} \emph{Scientific Reports} 13 (1): 15912.
\url{https://doi.org/10.1038/s41598-023-42814-7}.

\bibitem[\citeproctext]{ref-cacault_distance_2021}
Cacault, M Paula, Christian Hildebrand, Jérémy Laurent-Lucchetti, and
Michele Pellizzari. 2021. {``Distance {Learning} in {Higher}
{Education}: {Evidence} from a {Randomized} {Experiment}.''}
\emph{Journal of the European Economic Association} 19 (4): 2322--72.
\url{https://doi.org/10.1093/jeea/jvaa060}.

\bibitem[\citeproctext]{ref-escueta_education_2017}
Escueta, Maya, Vincent Quan, Andre Joshua Nickow, and Philip Oreopoulos.
2017. {``Education {Technology}: {An} {Evidence}-{Based} {Review},''}
August, w23744. \url{https://doi.org/10.3386/w23744}.

\bibitem[\citeproctext]{ref-fuchs-schundeln_covid-induced_2022}
Fuchs-Schündeln, Nicola. 2022. {``Covid-{Induced} {School} {Closures} in
the {US} and {Germany}: {Long}-{Term} {Distributional} {Effects}.''}
\emph{CESifo Working Paper Series}.
\url{https://ideas.repec.org//p/ces/ceswps/_9698.html}.

\bibitem[\citeproctext]{ref-grewenig_covid-19_2021}
Grewenig, Elisabeth, Philipp Lergetporer, Katharina Werner, Ludger
Woessmann, and Larissa Zierow. 2021. {``{COVID}-19 and Educational
Inequality: {How} School Closures Affect Low- and High-Achieving
Students.''} \emph{European Economic Review} 140 (November): 103920.
\url{https://doi.org/10.1016/j.euroecorev.2021.103920}.

\bibitem[\citeproctext]{ref-ives_did_2024}
Ives, Bob, and Ana-Maria Cazan. 2024. {``Did the {COVID}-19 Pandemic
Lead to an Increase in Academic Misconduct in Higher Education?''}
\emph{Higher Education} 87 (1): 111--29.
\url{https://doi.org/10.1007/s10734-023-00996-z}.

\bibitem[\citeproctext]{ref-jaeger_global_2021}
Jaeger, David A., Jaime Arellano-Bover, Krzysztof Karbownik, Marta
Martínez Matute, John M. Nunley, Jr Seals, Miguel Almunia, et al. 2021.
{``The {Global} {COVID}-19 {Student} {Survey}: {First} {Wave}
{Results}.''} Working \{Paper\} 14419. IZA Discussion Papers.
\url{https://www.econstor.eu/handle/10419/236450}.

\bibitem[\citeproctext]{ref-jaggars_how_2016}
Jaggars, Shanna Smith, and Di Xu. 2016. {``How Do Online Course Design
Features Influence Student Performance?''} \emph{Computers \& Education}
95 (April): 270--84.
\url{https://doi.org/10.1016/j.compedu.2016.01.014}.

\bibitem[\citeproctext]{ref-jenkins_when_2023}
Jenkins, Baylee D., Jonathan M. Golding, Alexis M. Le Grand, Mary M.
Levi, and Andrea M. Pals. 2023. {``When {Opportunity} {Knocks}:
{College} {Students}' {Cheating} {Amid} the {COVID}-19 {Pandemic}.''}
\emph{Teaching of Psychology} 50 (4): 407--19.
\url{https://doi.org/10.1177/00986283211059067}.

\bibitem[\citeproctext]{ref-joyce_does_2015}
Joyce, Ted, Sean Crockett, David A. Jaeger, Onur Altindag, and Stephen
D. O'Connell. 2015. {``Does Classroom Time Matter?''} \emph{Economics of
Education Review} 46 (June): 64--77.
\url{https://doi.org/10.1016/j.econedurev.2015.02.007}.

\bibitem[\citeproctext]{ref-kofoed_zooming_2021}
Kofoed, Michael S., Lucas Gebhart, Dallas Gilmore, and Ryan Moschitto.
2021. {``Zooming to {Class}?: {Experimental} {Evidence} on {College}
{Students}' {Online} {Learning} During {COVID}-19.''}
\url{https://www.iza.org/publications/dp/14356/zooming-to-class-experimental-evidence-on-college-students-online-learning-during-covid-19}.

\bibitem[\citeproctext]{ref-rodriguez-planas_hitting_2020}
Rodríguez-Planas, Núria. 2020. {``Hitting {Where} It {Hurts} {Most}:
{Covid}-19 and {Low}-{Income} {Urban} {College} {Students}.''} \{SSRN\}
\{Scholarly\} \{Paper\}. Rochester, NY.
\url{https://doi.org/10.2139/ssrn.3682958}.

\bibitem[\citeproctext]{ref-rodriguez-planas_covid-19_2022}
---------. 2022. {``{COVID}-19, College Academic Performance, and the
Flexible Grading Policy: {A} Longitudinal Analysis.''} \emph{Journal of
Public Economics} 207 (March): 104606.
\url{https://doi.org/10.1016/j.jpubeco.2022.104606}.

\bibitem[\citeproctext]{ref-walsh_why_2021}
Walsh, Lisa L., Deborah A. Lichti, Christina M. Zambrano-Varghese,
Ashish D. Borgaonkar, Jaskirat S. Sodhi, Swapnil Moon, Emma R. Wester,
and Kristine L. Callis-Duehl. 2021. {``Why and How Science Students in
the {United} {States} Think Their Peers Cheat More Frequently Online:
Perspectives During the {COVID}-19 Pandemic.''} \emph{International
Journal for Educational Integrity} 17 (1): 1--18.
\url{https://doi.org/10.1007/s40979-021-00089-3}.

\bibitem[\citeproctext]{ref-xu_promises_2019}
Xu, Di, and Ying Xu. 2019. {``The {Promises} and {Limits} of {Online}
{Higher} {Education}: {Understanding} {How} {Distance} {Education}
{Affects} {Access}, {Cost}, and {Quality}.''} American Enterprise
Institute. \url{https://eric.ed.gov/?id=ED596296}.

\end{CSLReferences}

\pagebreak

\section{Appendix}\label{appendix}

\subsection{Average final exam scores in ECO 1001 across
semesters}\label{average-final-exam-scores-in-eco-1001-across-semesters}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{working_paper_2_files/figure-pdf/exam-scores-mean-1.pdf}}

}

\caption{Average Final Exam Scores in ECO 1001 across Semesters}

\end{figure}%

\subsection{Student shares in low and high GPA
groups}\label{student-shares-in-low-and-high-gpa-groups}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{working_paper_2_files/figure-pdf/share-gpa-low-high-1.pdf}}

}

\caption{Share of High vs Low GPA Students}

\end{figure}%

\subsection{Average GPA in low and high GPA groups in ECO 1001 across
semesters}\label{average-gpa-in-low-and-high-gpa-groups-in-eco-1001-across-semesters}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{working_paper_2_files/figure-pdf/meangpa-gpa-group-1.pdf}}

}

\caption{Average GPA in High vs Low GPA Group of Students}

\end{figure}%

\subsection{Share of Students in hybrid and online
classes}\label{share-of-students-in-hybrid-and-online-classes}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{working_paper_2_files/figure-pdf/student-share-online-hybrid-1.pdf}}

}

\caption{Share of the Students in Hybrid vs Online Classes}

\end{figure}%

\pagebreak

\subsection{Example of a
Matched-Question}\label{example-of-a-matched-question}

As explained earlier, I was able to match 35 pairs of nearly identical
questions from pre-pandemic common exams to exams conducted during the
pandemic. I provide an example of one such question below that was
similar in common final exams in Fall 2019 and Fall 2020 which was
deemed to be \emph{hard} by the instructors. Full list of matched
questions are provided in a separate document.

\subsubsection{Fall 2019 version}\label{fall-2019-version}

\noindent Scenario 2, Monopoly: Let the following equations the market
for energy for ConEd, a monopolist: \(P=56-2Q\), \(MR=56-4Q\),
\(TC=50+6Q+3Q^2\), \(MC=6+6Q\)

\noindent Refer to Scenario 2, Monopoly: What is the profit of ConEd at
the profit maximizing quantity? (round to the nearest whole number and
pick the best answer)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  100
\item
  50
\item
  75
\item
  155
\end{enumerate}

\subsubsection{Fall 2020 version}\label{fall-2020-version}

\noindent A monopolist has a total cost curve represented by
\(TC=50+2Q+Q^2\), and a marginal cost curve represented by \(MC=2+2Q\).
The monopolist faces the demand curve \(P=100-3Q\). The price is in
dollars and the quantity is in thousands. What is the monopolist's
profit? (pick the closest answer)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  \$330,330
\item
  \$550,250
\item
  \$750,000
\item
  \$1,000,600
\end{enumerate}




\end{document}
